{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UXJlzyQiCBR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRjirQZQisIN"
      },
      "source": [
        "# **Ensayo SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D9-6igzi4dD"
      },
      "source": [
        "Support vector machines (SVMs)  son un conjunto de métodos de aprendizaje supervisado que se utilizan para la clasificación, la regresión y la detección de outliers\n",
        "\n",
        "**Las ventajas son:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Efectivo en espacios de gran dimensión.\n",
        "*   Sigue siendo eficaz en los casos en que el número de dimensiones es mayor que el número de muestras.\n",
        "*   Utiliza un subconjunto de puntos de entrenamiento en la función de decisión (llamados vectores de soporte), por lo que también es eficiente en memoria.\n",
        "*   Versátil: se pueden especificar diferentes funciones del kernel para la función de decisión. Se proporcionan kernels comunes, pero también es posible especificar kernels personalizados.\n",
        "\n",
        "\n",
        "**Las desventajas incluyen:**\n",
        "\n",
        "\n",
        "*   Si el número de características es mucho mayor que el número de muestras, evite el ajuste excesivo al elegir las funciones del Kernel y el término de regularización es crucial.\n",
        "*   Las SVM no proporcionan directamente estimaciones de probabilidad, estas se calculan mediante una costosa validación cruzada de five-fold.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stmiQBnZnTZN"
      },
      "source": [
        "# ¿Qué es Support Vector Machine?\n",
        "\n",
        "El objetivo del algoritmo de la máquina de vectores de soporte es encontrar un hiperplano en un espacio N-dimensional (N - el número de características) que clasifique claramente los puntos de datos.\n",
        "\n",
        "![](https://miro.medium.com/max/600/0*9jEWNXTAao7phK-5.png)\n",
        "![](https://miro.medium.com/max/600/0*0o8xIA4k3gXUDCFU.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAg4fwE5o5p-"
      },
      "source": [
        "Para separar las dos clases de puntos de datos, hay muchos hiperplanos posibles que podrían elegirse. Nuestro objetivo es encontrar un plano que tenga el margen máximo, es decir, la distancia máxima entre puntos de datos de ambas clases. Maximizar la distancia del margen proporciona cierto refuerzo para que los puntos de datos futuros se puedan clasificar con más confianza.\n",
        "\n",
        "Los hiperplanos son límites de decisión que ayudan a clasificar los puntos de datos. Los puntos de datos que caen a ambos lados del hiperplano se pueden atribuir a diferentes clases. Además, la dimensión del hiperplano depende del número de características. Si el número de entidades de entrada es 2, entonces el hiperplano es solo una línea. Si el número de entidades de entrada es 3, entonces el hiperplano se convierte en un plano bidimensional. Se vuelve difícil imaginar cuando el número de funciones supera las 3.\n",
        "\n",
        "![](https://miro.medium.com/max/700/0*ecA4Ls8kBYSM5nza.jpg)\n",
        "\n",
        "Los vectores de soporte son puntos de datos que están más cerca del hiperplano e influyen en la posición y orientación del hiperplano. Usando estos vectores de soporte, maximizamos el margen del clasificador. Eliminar los vectores de soporte cambiará la posición del hiperplano. Estos son los puntos que nos ayudan a construir nuestra SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBodGDDUs1yu"
      },
      "source": [
        "# The kernel trick\n",
        "\n",
        "SVM no necesita los vectores reales , en realidad solo necesita los productos punto entre ellos. Esto significa que podemos eludir los costosos cálculos de las nuevas dimensiones.\n",
        "\n",
        "\n",
        "Nuevo espacio que queremos: \n",
        "\n",
        "z = x² + y²\n",
        "\n",
        "Averigüe cómo se ve el producto punto en ese espacio:\n",
        "\n",
        "a · b = xa · xb  +  ya · yb  +  za · zb\n",
        "\n",
        "a · b = xa · xb  +  ya · yb +  (xa² + ya²) · (xb² + yb²)\n",
        "\n",
        "Indicarle a SVM que haga lo suyo, pero usando el nuevo producto punto, a esto lo llamamos función del kernel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzXoAYnqjFHg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}